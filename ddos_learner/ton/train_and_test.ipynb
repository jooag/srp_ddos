{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "while not os.path.exists('ddos_learner'):\n",
    "    os.chdir('../..')\n",
    "%matplotlib agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ddos_learner\n",
    "import ddos_learner.common\n",
    "import ddos_learner.ton.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river.tree import HoeffdingTreeClassifier, HoeffdingAdaptiveTreeClassifier, ExtremelyFastDecisionTreeClassifier\n",
    "from river.ensemble import SRPClassifier\n",
    "from river.forest import ARFClassifier\n",
    "from river.optim import Adam\n",
    "from river.linear_model import LogisticRegression, Perceptron, LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_ds = ddos_learner.ton.dataset.TONIoTDataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.reset()\n",
    "model = HoeffdingTreeClassifier()\n",
    "ddos_learner.common.train(model, train_ds, 1029538, step_size=10000, subpath='hdt_ton')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.reset()\n",
    "model = ARFClassifier()\n",
    "ddos_learner.common.train(model, train_ds, 1029538, step_size=10000, subpath='arf_ton')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.reset()\n",
    "model = HoeffdingAdaptiveTreeClassifier()\n",
    "ddos_learner.common.train(model, train_ds, 1029538, step_size=10000, subpath='hdt_adap_ton')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.reset()\n",
    "model = SRPClassifier(HoeffdingTreeClassifier())\n",
    "ddos_learner.common.train(model, train_ds, 1029538, step_size=10000, subpath='srp_hdt_ton')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=LogisticRegression(Adam(.1))\n",
    "train_ds.reset()\n",
    "ddos_learner.common.train(model, train_ds, 1029538, step_size=10000, subpath='log_reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training initiated... \n",
      "At 10000.\n",
      "Acc: 0.99804995. Prec: 0.99919906.\n",
      "Recl: 0.99884905. F1: 0.99902402. LB: 0.99870000\n",
      "At 20000.\n",
      "Acc: 0.99854996. Prec: 0.99919946.\n",
      "Recl: 0.99934946. F1: 0.99927446. LB: 0.99930000\n",
      "At 30000.\n",
      "Acc: 0.99854996. Prec: 0.99919946.\n",
      "Recl: 0.99934946. F1: 0.99927446. LB: 0.99950000\n",
      "At 40000.\n",
      "Acc: 0.99854996. Prec: 0.99919946.\n",
      "Recl: 0.99934946. F1: 0.99927446. LB: 0.99930000\n",
      "At 50000.\n",
      "Acc: 0.99739993. Prec: 0.99919854.\n",
      "Recl: 0.99819851. F1: 0.99869828. LB: 0.99580000\n",
      "At 60000.\n",
      "Acc: 0.99752494. Prec: 0.99919864.\n",
      "Recl: 0.99832362. F1: 0.99876094. LB: 0.99900000\n",
      "At 70000.\n",
      "Acc: 0.99739993. Prec: 0.99919854.\n",
      "Recl: 0.99819851. F1: 0.99869828. LB: 0.99950000\n",
      "At 80000.\n",
      "Acc: 0.99739993. Prec: 0.99919854.\n",
      "Recl: 0.99819851. F1: 0.99869828. LB: 0.99930000\n",
      "At 90000.\n",
      "Acc: 0.99739993. Prec: 0.99919854.\n",
      "Recl: 0.99819851. F1: 0.99869828. LB: 0.99930000\n",
      "At 100000.\n",
      "Acc: 0.99739993. Prec: 0.99919854.\n",
      "Recl: 0.99819851. F1: 0.99869828. LB: 0.99970000\n",
      "At 110000.\n",
      "Acc: 0.88324708. Prec: 0.99926448.\n",
      "Recl: 0.88380414. F1: 0.93799458. LB: 0.97510000\n",
      "At 120000.\n",
      "Acc: 0.93394835. Prec: 0.99914413.\n",
      "Recl: 0.93469612. F1: 0.96584622. LB: 0.97390000\n",
      "At 130000.\n",
      "Acc: 0.88579714. Prec: 0.99926659.\n",
      "Recl: 0.88635624. F1: 0.93943091. LB: 0.97550000\n",
      "At 140000.\n",
      "Acc: 0.93332333. Prec: 0.99914356.\n",
      "Recl: 0.93407061. F1: 0.96551189. LB: 0.97960000\n",
      "At 150000.\n",
      "Acc: 0.93392335. Prec: 0.99914411.\n",
      "Recl: 0.93467110. F1: 0.96583285. LB: 0.97540000\n",
      "At 160000.\n",
      "Acc: 0.88314708. Prec: 0.99926439.\n",
      "Recl: 0.88370406. F1: 0.93793818. LB: 0.97420000\n",
      "At 170000.\n",
      "Acc: 0.64691617. Prec: 0.99899598.\n",
      "Recl: 0.64728401. F1: 0.78557005. LB: 0.97520000\n",
      "At 180000.\n",
      "Acc: 0.93202330. Prec: 0.99914237.\n",
      "Recl: 0.93276953. F1: 0.96481580. LB: 0.97370000\n",
      "At 190000.\n",
      "Acc: 0.57251431. Prec: 0.99860457.\n",
      "Recl: 0.57297270. F1: 0.72815148. LB: 0.97450000\n",
      "At 200000.\n",
      "Acc: 0.57506438. Prec: 0.99861075.\n",
      "Recl: 0.57552481. F1: 0.73021063. LB: 0.97440000\n",
      "At 210000.\n",
      "Acc: 0.99919998. Prec: 0.99919998.\n",
      "Recl: 1.00000000. F1: 0.99959983. LB: 0.99680000\n",
      "At 220000.\n",
      "Acc: 0.99919998. Prec: 0.99919998.\n",
      "Recl: 1.00000000. F1: 0.99959983. LB: 0.99790000\n",
      "At 230000.\n",
      "Acc: 0.86032151. Prec: 0.99912892.\n",
      "Recl: 0.86096029. F1: 0.92491298. LB: 0.97620000\n",
      "At 240000.\n",
      "Acc: 0.86722168. Prec: 0.99913585.\n",
      "Recl: 0.86786599. F1: 0.92888609. LB: 0.93050000\n",
      "At 250000.\n",
      "Acc: 0.93759844. Prec: 0.99920066.\n",
      "Recl: 0.93829910. F1: 0.96779272. LB: 0.91710000\n",
      "At 260000.\n",
      "Acc: 0.84774619. Prec: 0.99911601.\n",
      "Recl: 0.84837491. F1: 0.91759580. LB: 0.86840000\n",
      "At 270000.\n",
      "Acc: 0.93522338. Prec: 0.99919863.\n",
      "Recl: 0.93592214. F1: 0.96652585. LB: 0.99600000\n",
      "At 280000.\n",
      "Acc: 0.86282157. Prec: 0.99913144.\n",
      "Recl: 0.86346236. F1: 0.92635591. LB: 0.99020000\n",
      "At 290000.\n",
      "Acc: 0.93362334. Prec: 0.99919726.\n",
      "Recl: 0.93432081. F1: 0.96567062. LB: 0.99450000\n",
      "At 300000.\n",
      "Acc: 0.93432336. Prec: 0.99919786.\n",
      "Recl: 0.93502139. F1: 0.96604495. LB: 0.99680000\n",
      "At 310000.\n",
      "Acc: 0.59668992. Prec: 0.99866103.\n",
      "Recl: 0.59716766. F1: 0.74740864. LB: 0.83150000\n",
      "At 320000.\n",
      "Acc: 0.36248406. Prec: 0.99779781.\n",
      "Recl: 0.36277429. F1: 0.53209292. LB: 0.96630000\n",
      "At 330000.\n",
      "Acc: 0.36113403. Prec: 0.99778960.\n",
      "Recl: 0.36142317. F1: 0.53063698. LB: 0.96850000\n",
      "At 340000.\n",
      "Acc: 0.62789070. Prec: 0.99872748.\n",
      "Recl: 0.62839342. F1: 0.77141629. LB: 0.96580000\n",
      "At 350000.\n",
      "Acc: 0.60779019. Prec: 0.99868545.\n",
      "Recl: 0.60827683. F1: 0.75605660. LB: 0.97100000\n",
      "At 360000.\n",
      "Acc: 0.50523763. Prec: 0.99841905.\n",
      "Recl: 0.50564215. F1: 0.67130614. LB: 0.96750000\n",
      "At 370000.\n",
      "Acc: 0.35083377. Prec: 0.99772485.\n",
      "Recl: 0.35111467. F1: 0.51943293. LB: 0.96860000\n",
      "At 380000.\n",
      "Acc: 0.59463987. Prec: 0.99865642.\n",
      "Recl: 0.59511597. F1: 0.74579832. LB: 0.96940000\n",
      "At 390000.\n",
      "Acc: 0.43971099. Prec: 0.99863667.\n",
      "Recl: 0.43986289. F1: 0.61072415. LB: 0.89680000\n",
      "At 400000.\n",
      "Acc: 0.84744619. Prec: 0.99911570.\n",
      "Recl: 0.84807466. F1: 0.91742002. LB: 0.99750000\n",
      "At 410000.\n",
      "Acc: 0.84772119. Prec: 0.99911598.\n",
      "Recl: 0.84834989. F1: 0.91758115. LB: 0.99840000\n",
      "At 420000.\n",
      "Acc: 0.84774619. Prec: 0.99911601.\n",
      "Recl: 0.84837491. F1: 0.91759580. LB: 0.99790000\n",
      "At 430000.\n",
      "Acc: 0.93524838. Prec: 0.99919865.\n",
      "Recl: 0.93594716. F1: 0.96653920. LB: 0.99850000\n",
      "At 440000.\n",
      "Acc: 0.60859021. Prec: 0.99876903.\n",
      "Recl: 0.60902745. F1: 0.75666014. LB: 0.99710000\n",
      "At 450000.\n",
      "Acc: 0.84734618. Prec: 0.99911559.\n",
      "Recl: 0.84797458. F1: 0.91736141. LB: 0.99920000\n",
      "At 460000.\n",
      "Acc: 0.84757119. Prec: 0.99911583.\n",
      "Recl: 0.84819976. F1: 0.91749327. LB: 0.99810000\n",
      "At 470000.\n",
      "Acc: 0.84934623. Prec: 0.99911767.\n",
      "Recl: 0.84997623. F1: 0.91853234. LB: 0.99720000\n",
      "At 480000.\n",
      "Acc: 0.93522338. Prec: 0.99919863.\n",
      "Recl: 0.93592214. F1: 0.96652585. LB: 0.99880000\n",
      "At 490000.\n",
      "Acc: 0.25348134. Prec: 0.99704928.\n",
      "Recl: 0.25363425. F1: 0.40439622. LB: 0.89060000\n",
      "At 500000.\n",
      "Acc: 0.20085502. Prec: 0.99701863.\n",
      "Recl: 0.20081567. F1: 0.33429827. LB: 0.00000000\n",
      "At 510000.\n",
      "Acc: 0.20085502. Prec: 0.99701863.\n",
      "Recl: 0.20081567. F1: 0.33429827. LB: 0.00000000\n",
      "At 520000.\n",
      "Acc: 0.20085502. Prec: 0.99701863.\n",
      "Recl: 0.20081567. F1: 0.33429827. LB: 0.53970000\n",
      "At 530000.\n",
      "Acc: 0.20085502. Prec: 0.99701863.\n",
      "Recl: 0.20081567. F1: 0.33429827. LB: 0.90950000\n",
      "At 540000.\n",
      "Acc: 0.41933548. Prec: 0.99857049.\n",
      "Recl: 0.41947106. F1: 0.59077454. LB: 0.99710000\n",
      "At 550000.\n",
      "Acc: 0.46656166. Prec: 0.99871507.\n",
      "Recl: 0.46673506. F1: 0.63616677. LB: 0.99890000\n",
      "At 560000.\n",
      "Acc: 0.47748694. Prec: 0.99874444.\n",
      "Recl: 0.47766908. F1: 0.64625436. LB: 0.99910000\n",
      "At 570000.\n",
      "Acc: 0.55426386. Prec: 0.99864853.\n",
      "Recl: 0.55465759. F1: 0.71319875. LB: 0.99880000\n",
      "At 580000.\n",
      "Acc: 0.55948899. Prec: 0.99866113.\n",
      "Recl: 0.55988691. F1: 0.71751050. LB: 0.99840000\n",
      "At 590000.\n",
      "Acc: 0.56041401. Prec: 0.99866334.\n",
      "Recl: 0.56081267. F1: 0.71827082. LB: 0.99960000\n",
      "At 600000.\n",
      "Acc: 0.56136403. Prec: 0.99866560.\n",
      "Recl: 0.56176345. F1: 0.71905075. LB: 0.99970000\n",
      "At 610000.\n",
      "Acc: 0.56603915. Prec: 0.99867661.\n",
      "Recl: 0.56644231. F1: 0.72287502. LB: 0.99940000\n",
      "At 620000.\n",
      "Acc: 0.58148954. Prec: 0.99871173.\n",
      "Recl: 0.58190507. F1: 0.73535270. LB: 0.99910000\n",
      "At 630000.\n",
      "Acc: 0.59781495. Prec: 0.99874687.\n",
      "Recl: 0.59824355. F1: 0.74827484. LB: 0.99780000\n",
      "At 640000.\n",
      "Acc: 0.60741519. Prec: 0.99876665.\n",
      "Recl: 0.60785148. F1: 0.75575119. LB: 0.99940000\n",
      "At 650000.\n",
      "Acc: 0.60826521. Prec: 0.99876837.\n",
      "Recl: 0.60870218. F1: 0.75640886. LB: 0.99950000\n",
      "At 660000.\n",
      "Acc: 0.60859021. Prec: 0.99876903.\n",
      "Recl: 0.60902745. F1: 0.75666014. LB: 0.99970000\n",
      "At 670000.\n",
      "Acc: 0.60976524. Prec: 0.99877140.\n",
      "Recl: 0.61020342. F1: 0.75756776. LB: 0.99870000\n",
      "At 680000.\n",
      "Acc: 0.60991525. Prec: 0.99877170.\n",
      "Recl: 0.61035354. F1: 0.75768353. LB: 0.98530000\n",
      "At 690000.\n",
      "Acc: 0.60886522. Prec: 0.99876958.\n",
      "Recl: 0.60930267. F1: 0.75687268. LB: 0.93240000\n",
      "At 700000.\n",
      "Acc: 0.60949024. Prec: 0.99877084.\n",
      "Recl: 0.60992819. F1: 0.75735545. LB: 0.97050000\n",
      "At 710000.\n",
      "Acc: 0.60476512. Prec: 0.99876125.\n",
      "Recl: 0.60519929. F1: 0.75369635. LB: 0.97230000\n",
      "At 720000.\n",
      "Acc: 0.60779019. Prec: 0.99876741.\n",
      "Recl: 0.60822679. F1: 0.75604143. LB: 0.97280000\n",
      "At 730000.\n",
      "Acc: 0.60776519. Prec: 0.99876736.\n",
      "Recl: 0.60820177. F1: 0.75602208. LB: 0.97520000\n",
      "At 740000.\n",
      "Acc: 0.60784020. Prec: 0.99876751.\n",
      "Recl: 0.60827683. F1: 0.75608011. LB: 0.97390000\n",
      "At 750000.\n",
      "Acc: 0.60126503. Prec: 0.99875405.\n",
      "Recl: 0.60169640. F1: 0.75097197. LB: 0.83740000\n",
      "At 760000.\n",
      "Acc: 0.60966524. Prec: 0.99877120.\n",
      "Recl: 0.61010334. F1: 0.75749056. LB: 0.99990000\n",
      "At 770000.\n",
      "Acc: 0.60969024. Prec: 0.99877125.\n",
      "Recl: 0.61012836. F1: 0.75750986. LB: 0.99980000\n",
      "At 780000.\n",
      "Acc: 0.60971524. Prec: 0.99877130.\n",
      "Recl: 0.61015338. F1: 0.75752916. LB: 0.99940000\n",
      "At 790000.\n",
      "Acc: 0.61006525. Prec: 0.99877200.\n",
      "Recl: 0.61050367. F1: 0.75779928. LB: 0.99980000\n",
      "At 800000.\n",
      "Acc: 0.61026526. Prec: 0.99877240.\n",
      "Recl: 0.61070383. F1: 0.75795358. LB: 0.99930000\n",
      "At 810000.\n",
      "Acc: 0.61054026. Prec: 0.99877296.\n",
      "Recl: 0.61097906. F1: 0.75816567. LB: 0.99830000\n",
      "At 820000.\n",
      "Acc: 0.61116528. Prec: 0.99877421.\n",
      "Recl: 0.61160457. F1: 0.75864744. LB: 0.99850000\n",
      "At 830000.\n",
      "Acc: 0.61144029. Prec: 0.99877476.\n",
      "Recl: 0.61187980. F1: 0.75885931. LB: 0.99840000\n",
      "At 840000.\n",
      "Acc: 0.61041526. Prec: 0.99877270.\n",
      "Recl: 0.61085395. F1: 0.75806927. LB: 0.99810000\n",
      "At 850000.\n",
      "Acc: 0.61014025. Prec: 0.99877215.\n",
      "Recl: 0.61057873. F1: 0.75785714. LB: 0.22820000\n",
      "At 860000.\n",
      "Acc: 0.60969024. Prec: 0.99877125.\n",
      "Recl: 0.61012836. F1: 0.75750986. LB: 0.00000000\n",
      "At 870000.\n",
      "Acc: 0.57836446. Prec: 0.99870478.\n",
      "Recl: 0.57877749. F1: 0.73284861. LB: 0.42080000\n",
      "At 880000.\n",
      "Acc: 0.59871497. Prec: 0.99874875.\n",
      "Recl: 0.59914429. F1: 0.74897956. LB: 0.99640000\n",
      "At 890000.\n",
      "Acc: 0.59996500. Prec: 0.99875135.\n",
      "Recl: 0.60039533. F1: 0.74995703. LB: 0.97230000\n",
      "At 900000.\n",
      "Acc: 0.60766519. Prec: 0.99876716.\n",
      "Recl: 0.60810168. F1: 0.75594470. LB: 0.93750000\n",
      "At 910000.\n",
      "Acc: 0.60501513. Prec: 0.99876176.\n",
      "Recl: 0.60544950. F1: 0.75389049. LB: 0.74550000\n",
      "At 920000.\n",
      "Acc: 0.60996525. Prec: 0.99877180.\n",
      "Recl: 0.61040358. F1: 0.75772211. LB: 0.99960000\n",
      "At 930000.\n",
      "Acc: 0.60929023. Prec: 0.99877044.\n",
      "Recl: 0.60972803. F1: 0.75720101. LB: 0.85600000\n",
      "At 940000.\n",
      "Acc: 0.59688992. Prec: 0.99874493.\n",
      "Recl: 0.59731779. F1: 0.74754971. LB: 0.97450000\n",
      "At 950000.\n",
      "Acc: 0.60749019. Prec: 0.99876680.\n",
      "Recl: 0.60792654. F1: 0.75580925. LB: 0.97410000\n",
      "At 960000.\n",
      "Acc: 0.60809020. Prec: 0.99876802.\n",
      "Recl: 0.60852703. F1: 0.75627352. LB: 0.96830000\n",
      "At 970000.\n",
      "Acc: 0.60831521. Prec: 0.99876847.\n",
      "Recl: 0.60875222. F1: 0.75644753. LB: 0.96960000\n",
      "At 980000.\n",
      "Acc: 0.60881522. Prec: 0.99876948.\n",
      "Recl: 0.60925263. F1: 0.75683404. LB: 0.96970000\n",
      "At 990000.\n",
      "Acc: 0.70651766. Prec: 0.99886894.\n",
      "Recl: 0.70708334. F1: 0.82802268. LB: 0.96870000\n",
      "At 1000000.\n",
      "Acc: 0.77721943. Prec: 0.99897172.\n",
      "Recl: 0.77784172. F1: 0.87464656. LB: 0.96960000\n",
      "At 1010000.\n",
      "Acc: 0.76444411. Prec: 0.99895456.\n",
      "Recl: 0.76505617. F1: 0.86649853. LB: 0.96930000\n",
      "At 1020000.\n",
      "Acc: 0.76564414. Prec: 0.99895619.\n",
      "Recl: 0.76625716. F1: 0.86726892. LB: 0.97190000\n",
      "Resetting dataset.\n"
     ]
    }
   ],
   "source": [
    "train_ds.reset()\n",
    "model = Perceptron()\n",
    "ddos_learner.common.train(model, train_ds, 1029538, step_size=10000, subpath='perc_ton')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.reset()\n",
    "for i in [1, 2, 4, 6, 8, 10, 12]:\n",
    "    model = SRPClassifier(HoeffdingTreeClassifier(), i)\n",
    "    ddos_learner.common.train(model, train_ds, 1029538, step_size=10000, subpath=f'srp_{i}_hdt_ton')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.reset()\n",
    "for subspace_size in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n",
    "    model = SRPClassifier(HoeffdingTreeClassifier(), subspace_size=subspace_size)\n",
    "    ddos_learner.common.train(model, train_ds, 1029538, step_size=10000, subpath=f'srp_10_{subspace_size:.2f}_hdt_ton')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e1d9a8909477db77738c33245c29c7265277ef753467dede8cf3f814cde494e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
